{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "import nonlocal_resnet_v1_50_nl3 as model\n",
    "import config\n",
    "import tensorflow as tf\n",
    "\n",
    "g1 = tf.Graph()\n",
    "with g1.as_default() as g:    \n",
    "    images = tf.placeholder(dtype=tf.float32, shape=(config.batch_size, config.n_frames,224,224,3))\n",
    "    segs = tf.placeholder(dtype=tf.float32, shape=(config.batch_size, config.n_frames,224,224,1))\n",
    "    curr_key = tf.placeholder(dtype=tf.float32, shape=(config.batch_size, config.n_frames,7,7,128))\n",
    "    curr_value = tf.placeholder(dtype=tf.float32, shape=(config.batch_size, config.n_frames,7,7,512))\n",
    "    mem_key, mem_value, mem_net = model.mem_encoder(images, segs, is_training=False)   \n",
    "#     curr_key, curr_value, curr_net, block1, block2, block3= model.curr_encoder(images, is_training=False)\n",
    "    attn_net  = model.attention(mem_key, mem_value, curr_key, curr_value)\n",
    "#     out_seg = model.curr_decoder(curr_net, block1, block2, block3)\n",
    "    model_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'resnet_v1_50/(?!sc)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session(graph=g1) as sess:    \n",
    "    saver = tf.train.Saver(model_variables)\n",
    "    saver.restore(sess, 'resnet_v1_50.ckpt')\n",
    "    writer = tf.summary.FileWriter('./logs/summary', sess.graph)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sc/attention/concat:0' shape=(2, 4, 7, 7, 1024) dtype=float32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(mem_net.get_shape()[-1])/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_key, curr_value,  curr_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints['model_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim.nets import resnet_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = tf.placeholder(tf.float32, shape=[2,224,224,3])\n",
    "with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "    net, features = resnet_v1.resnet_v1_50(ip, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[list(features.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.pop('resnet_v1_50/conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer =  'resnet_v1_50/block4/unit_3/bottleneck_v1/conv3'\n",
    "updated_layer_tensor = tf.concat([features.get(layer),features.get(layer)],axis = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'resnet_v1_50/conv1'\n",
    "inp = tf.placeholder(tf.float32, shape=[2,224,224,3])\n",
    "conv1 = features.get(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = resnet_v1.resnet_v1_block('block1', base_depth=64, num_units=3, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 256\n",
    "weight_decay = 0.0001\n",
    "batch_norm_decay = 0.999\n",
    "batch_renorm_decay = 0.99\n",
    "batch_renorm_rmax = 3.\n",
    "batch_renorm_dmax = 5.\n",
    "is_training = False\n",
    "use_conv3d = False\n",
    "from tensorflow.contrib.slim.python.slim.nets import resnet_v1\n",
    "\n",
    "def convert_resnet_arg_scope_to_slim(resnet_arg_scope):\n",
    "    arg_scope = {}\n",
    "    for k, v in resnet_arg_scope.items():\n",
    "        v = dict(v)\n",
    "        if 'batch_norm' in k:\n",
    "            k = slim.batch_norm\n",
    "        elif 'max_pool2d' in k:\n",
    "            k = slim.max_pool2d\n",
    "        elif 'convolution' in k:\n",
    "            k = slim.conv2d\n",
    "            v['normalizer_fn'] = slim.batch_norm\n",
    "        arg_scope[k] = v\n",
    "    return arg_scope\n",
    "    \n",
    "resnet_arg_scope = resnet_v1.resnet_arg_scope()\n",
    "\n",
    "import tensorflow as tf\n",
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "    inp = tf.placeholder(tf.float32, shape=[2,224,224,3])\n",
    "    with slim.arg_scope(resnet_arg_scope):\n",
    "        blocks = [\n",
    "            resnet_v1.resnet_v1_block('block1', base_depth=64, num_units=3, stride=2),\n",
    "            resnet_v1.resnet_v1_block('block2', base_depth=128, num_units=4, stride=2),\n",
    "            resnet_v1.resnet_v1_block('block3', base_depth=256, num_units=6, stride=2),\n",
    "            resnet_v1.resnet_v1_block('block4', base_depth=512, num_units=3, stride=1),\n",
    "        ]\n",
    "        _, end_points = resnet_v1.resnet_v1(inp, blocks,\n",
    "                                            is_training=is_training,\n",
    "                                            include_root_block=False)\n",
    "    net = end_points\n",
    "\n",
    "    arg_scope = convert_resnet_arg_scope_to_slim(resnet_arg_scope)\n",
    "    arg_scope[slim.batch_norm]['is_training'] = is_training\n",
    "\n",
    "# g1 = tf.Graph()\n",
    "# with g1.as_default() as g:\n",
    "#     with slim.arg_scope(arg_scope):\n",
    "#         feature_map = slim.conv2d(net, dim, [1,1],\n",
    "#                                   activation_fn=None,\n",
    "#                                   normalizer_fn=None)\n",
    "\n",
    "    \n",
    "# import tensorflow as tf\n",
    "\n",
    "# g1 = tf.Graph()\n",
    "# with g1.as_default() as g:\n",
    "#     with g.name_scope(\"resnet\") as g1_scope:\n",
    "#         inp = tf.placeholder(tf.float32, shape=[2,384,384,3])\n",
    "#         with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "#             _, features = resnet_v1.resnet_v1_50(inp)\n",
    "        \n",
    "tf.reset_default_graph()\n",
    "with tf.Session(graph=g1) as sess:\n",
    "    writer = tf.summary.FileWriter('./logs/summary', sess.graph)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_v1.layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_v1.resnet_v1_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_points['resnet_v1_50/block4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = slim.get_trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "g1 = tf.Graph()\n",
    "with g1.as_default() as g:\n",
    "    with g.name_scope(\"resnet\") as g1_scope:\n",
    "        inp = tf.placeholder(tf.float32, shape=[2,384,384,3])\n",
    "        with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "            _, features = resnet_v1.resnet_v1_50(inp)\n",
    "        \n",
    "tf.reset_default_graph()\n",
    "\n",
    "scope = g1_scope\n",
    "\n",
    "with tf.Session(graph = g1) as session:\n",
    "    tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.placeholder(tf.float32, shape=[2,384,384,3])\n",
    "        with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "            _, features = resnet_v1.resnet_v1_50(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "    inp = tf.placeholder(tf.float32, shape=[2,224,224,3])\n",
    "    with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "        net, features = resnet_v1.resnet_v1_50(inp)\n",
    "#     feature_map = feature_extractor_resnet(inp,\n",
    "#                                          dim = 256,\n",
    "#                                          weight_decay = 0.0001,\n",
    "#                                          batch_norm_decay = 0.999,\n",
    "#                                          batch_renorm_decay = 0.99,\n",
    "#                                          batch_renorm_rmax = 3.,\n",
    "#                                          batch_renorm_dmax = 5.,\n",
    "#                                          is_training = False,\n",
    "#                                          use_conv3d = False)\n",
    "\n",
    "with tf.Session(graph=g1) as sess:\n",
    "    writer = tf.summary.FileWriter('./logs/summary', sess.graph)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "def convert_resnet_arg_scope_to_slim(resnet_arg_scope):\n",
    "    arg_scope = {}\n",
    "    for k, v in resnet_arg_scope.items():\n",
    "        v = dict(v)\n",
    "        if 'batch_norm' in k:\n",
    "            k = slim.batch_norm\n",
    "        elif 'max_pool2d' in k:\n",
    "            k = slim.max_pool2d\n",
    "        elif 'convolution' in k:\n",
    "            k = slim.conv2d\n",
    "            v['normalizer_fn'] = slim.batch_norm\n",
    "        arg_scope[k] = v\n",
    "    arg_scope[slim.conv3d] = arg_scope[slim.conv2d]\n",
    "    return arg_scope\n",
    "\n",
    "def feature_extractor_resnet(images,\n",
    "                             dim = 256,\n",
    "                             weight_decay = 0.0001,\n",
    "                             batch_norm_decay = 0.999,\n",
    "                             batch_renorm_decay = 0.99,\n",
    "                             batch_renorm_rmax = 3.,\n",
    "                             batch_renorm_dmax = 5.,\n",
    "                             is_training = False,\n",
    "                             use_conv3d = False):\n",
    "    from tensorflow.contrib.slim.python.slim.nets import resnet_v1\n",
    "    if use_conv3d:\n",
    "        orig_shape = tf.shape(images)\n",
    "        # [N,T,H,W,C] -> [N*T,H,W,C]\n",
    "        images = tf.reshape(images, tf.concat([[-1], orig_shape[2:]], 0))\n",
    "\n",
    "    resnet_arg_scope = resnet_v1.resnet_arg_scope(weight_decay=weight_decay,\n",
    "                                                  batch_norm_decay=batch_norm_decay)\n",
    "    # batch size is small so we use batch renormalization\n",
    "#     batch_norm_key = filter(lambda x: 'batch_norm' in x, resnet_arg_scope.keys())[0]\n",
    "#     resnet_arg_scope[batch_norm_key].update({'renorm': True,\n",
    "#                                              'renorm_decay': batch_renorm_decay,\n",
    "#                                              'renorm_clipping': {'rmin': 1./batch_renorm_rmax,\n",
    "#                                                                  'rmax': batch_renorm_rmax,\n",
    "#                                                                  'dmax': batch_renorm_dmax}})\n",
    "    \n",
    "    with slim.arg_scope(resnet_arg_scope):\n",
    "        blocks = [\n",
    "            resnet_v1.resnet_v1_block('block1', base_depth=16, num_units=3, stride=2),\n",
    "            resnet_v1.resnet_v1_block('block2', base_depth=32, num_units=4, stride=2),\n",
    "            resnet_v1.resnet_v1_block('block3', base_depth=64, num_units=6, stride=2), #256\n",
    "            resnet_v1.resnet_v1_block('block4', base_depth=128, num_units=3, stride=1) #512\n",
    "        ]\n",
    "        _, end_points = resnet_v1.resnet_v1(images, blocks,\n",
    "                                            is_training=is_training,\n",
    "                                            include_root_block=True)\n",
    "    net = end_points['resnet_v1/block4']\n",
    "    if use_conv3d:\n",
    "        # [N*T,H',W',C'] -> [N,T,H',W',C']\n",
    "        net = tf.reshape(net, tf.concat([orig_shape[:2], tf.shape(net)[1:]], 0))\n",
    "\n",
    "    arg_scope = convert_resnet_arg_scope_to_slim(resnet_arg_scope)\n",
    "    arg_scope[slim.conv2d].update({'stride': 1, 'padding': 'SAME'})\n",
    "    arg_scope[slim.conv3d].update({'stride': 1, 'padding': 'SAME'})\n",
    "    arg_scope[slim.batch_norm]['is_training'] = is_training\n",
    "    with slim.arg_scope(arg_scope):\n",
    "        if use_conv3d:\n",
    "            net = slim.conv3d(net, 512, [3,3,3])\n",
    "            net = slim.conv3d(net, 256, [1,1,1])\n",
    "            net = slim.conv3d(net, 512, [3,3,3])\n",
    "            # the last layer without activation function\n",
    "            feature_map = slim.conv3d(net, dim, [1,1,1],\n",
    "                                      activation_fn=None,\n",
    "                                      normalizer_fn=None)\n",
    "        else:\n",
    "            # the last layer without activation function\n",
    "            feature_map = slim.conv2d(net, dim, [1,1],\n",
    "                                      activation_fn=None,\n",
    "                                      normalizer_fn=None)\n",
    "    return feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim.get_trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib as tf_contrib\n",
    "\n",
    "def conv(inputs, name, features, is_seg_mask=False, padding='SAME'):\n",
    "    filt = filter(name, features, is_seg_mask)\n",
    "    x = tf.nn.conv2d(inputs, filt, [1, 1, 1, 1], padding=padding)\n",
    "    return x\n",
    "\n",
    "def filter(name, features, is_seg_mask=False):\n",
    "    if is_seg_mask:\n",
    "        loaded_weights = features[name]\n",
    "        out = tf.random_uniform(tf.reduce_mean(loaded_weights, 2, True).get_shape().as_list())\n",
    "        return tf.Variable(tf.concat((loaded_weights, out), axis=2), name=\"filter\")\n",
    "    else:\n",
    "        return tf.Variable(features[name], name=\"filter\")\n",
    "\n",
    "def relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def batch_norm(x, is_training=False):\n",
    "    return tf_contrib.layers.batch_norm(x,\n",
    "                                        decay=0.9, epsilon=1e-05,\n",
    "                                        center=True, scale=True, updates_collections=None,\n",
    "                                        is_training=is_training)\n",
    "\n",
    "def bottle_resblock(x_init,  channels, is_training=False, use_bias=True, downsample=False, scope='bottle_resblock'):\n",
    "    with tf.variable_scope(scope):\n",
    "        x = batch_norm(x_init, is_training, scope='batch_norm_1x1_front')\n",
    "        shortcut = relu(x)\n",
    "\n",
    "        x = conv(shortcut, name, channels, kernel=1, stride=1, use_bias=use_bias, scope='conv_1x1_front')\n",
    "        x = batch_norm(x, is_training, scope='batch_norm_3x3')\n",
    "        x = relu(x)\n",
    "\n",
    "        if downsample:\n",
    "            x = conv(x, channels, kernel=3, stride=2, use_bias=use_bias, scope='conv_0')\n",
    "            shortcut = conv(shortcut, channels * 4, kernel=1, stride=2, use_bias=use_bias, scope='conv_init')\n",
    "\n",
    "        else:\n",
    "            x = conv(x, channels, kernel=3, stride=1, use_bias=use_bias, scope='conv_0')\n",
    "            shortcut = conv(shortcut, channels * 4, kernel=1, stride=1, use_bias=use_bias, scope='conv_init')\n",
    "\n",
    "        x = batch_norm(x, is_training, scope='batch_norm_1x1_back')\n",
    "        x = relu(x)\n",
    "        x = conv(x, channels * 4, kernel=1, stride=1, use_bias=use_bias, scope='conv_1x1_back')\n",
    "\n",
    "        return x + shortcut\n",
    "\n",
    "def get_residual_layer(res_n):\n",
    "    x = []\n",
    "\n",
    "    if res_n == 18:\n",
    "        x = [2, 2, 2, 2]\n",
    "\n",
    "    if res_n == 34:\n",
    "        x = [3, 4, 6, 3]\n",
    "\n",
    "    if res_n == 50:\n",
    "        x = [3, 4, 6, 3]\n",
    "\n",
    "    if res_n == 101:\n",
    "        x = [3, 4, 23, 3]\n",
    "\n",
    "    if res_n == 152:\n",
    "        x = [3, 8, 36, 3]\n",
    "\n",
    "    return x\n",
    "\n",
    "def resnet_network(x, layer_names, features, is_training=False, reuse=False):\n",
    "    with tf.variable_scope(\"resnet\", reuse=reuse):\n",
    "\n",
    "        residual_block = bottle_resblock\n",
    "\n",
    "        residual_list = get_residual_layer(50)\n",
    "\n",
    "        ch = 64\n",
    "\n",
    "        x = conv(x, layer_names[0], features, True)\n",
    "\n",
    "        for i in range(residual_list[0]):\n",
    "            x = residual_block(x, channels=ch, is_training=is_training, downsample=False,\n",
    "                               scope='resblock0_' + str(i))\n",
    "\n",
    "        ########################################################################################################\n",
    "\n",
    "        x = residual_block(x, channels=ch * 2, is_training=is_training, downsample=True, scope='resblock1_0')\n",
    "\n",
    "        for i in range(1, residual_list[1]):\n",
    "            x = residual_block(x, channels=ch * 2, is_training=is_training, downsample=False,\n",
    "                               scope='resblock1_' + str(i))\n",
    "\n",
    "        ########################################################################################################\n",
    "\n",
    "        x = residual_block(x, channels=ch * 4, is_training=is_training, downsample=True, scope='resblock2_0')\n",
    "\n",
    "        for i in range(1, residual_list[2]):\n",
    "            x = residual_block(x, channels=ch * 4, is_training=is_training, downsample=False,\n",
    "                               scope='resblock2_' + str(i))\n",
    "\n",
    "        ########################################################################################################\n",
    "\n",
    "        x = residual_block(x, channels=ch * 8, is_training=is_training, downsample=True, scope='resblock_3_0')\n",
    "\n",
    "        for i in range(1, residual_list[3]):\n",
    "            x = residual_block(x, channels=ch * 8, is_training=is_training, downsample=False,\n",
    "                               scope='resblock_3_' + str(i))\n",
    "\n",
    "        ########################################################################################################\n",
    "\n",
    "        x = batch_norm(x, is_training, scope='batch_norm')\n",
    "        x = relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_arg_scope(weight_decay=0.0001,\n",
    "                     batch_norm_decay=0.997,\n",
    "                     batch_norm_epsilon=1e-5,\n",
    "                     batch_norm_scale=True,\n",
    "                     activation_fn=tf.nn.relu,\n",
    "                     use_batch_norm=True,\n",
    "                     batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS):\n",
    "  \"\"\"Defines the default ResNet arg scope.\n",
    "  TODO(gpapan): The batch-normalization related default values above are\n",
    "    appropriate for use in conjunction with the reference ResNet models\n",
    "    released at https://github.com/KaimingHe/deep-residual-networks. When\n",
    "    training ResNets from scratch, they might need to be tuned.\n",
    "  Args:\n",
    "    weight_decay: The weight decay to use for regularizing the model.\n",
    "    batch_norm_decay: The moving average decay when estimating layer activation\n",
    "      statistics in batch normalization.\n",
    "    batch_norm_epsilon: Small constant to prevent division by zero when\n",
    "      normalizing activations by their variance in batch normalization.\n",
    "    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n",
    "      activations in the batch normalization layer.\n",
    "    activation_fn: The activation function which is used in ResNet.\n",
    "    use_batch_norm: Whether or not to use batch normalization.\n",
    "    batch_norm_updates_collections: Collection for the update ops for\n",
    "      batch norm.\n",
    "  Returns:\n",
    "    An `arg_scope` to use for the resnet models.\n",
    "  \"\"\"\n",
    "  batch_norm_params = {\n",
    "      'decay': batch_norm_decay,\n",
    "      'epsilon': batch_norm_epsilon,\n",
    "      'scale': batch_norm_scale,\n",
    "      'updates_collections': batch_norm_updates_collections,\n",
    "      'fused': None,  # Use fused batch norm if possible.\n",
    "  }\n",
    "\n",
    "  with slim.arg_scope(\n",
    "      [slim.conv2d],\n",
    "      weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "      weights_initializer=slim.variance_scaling_initializer(),\n",
    "      activation_fn=activation_fn,\n",
    "      normalizer_fn=slim.batch_norm if use_batch_norm else None,\n",
    "      normalizer_params=batch_norm_params):\n",
    "    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n",
    "      # The following implies padding='SAME' for pool1, which makes feature\n",
    "      # alignment easier for dense prediction tasks. This is also used in\n",
    "      # https://github.com/facebook/fb.resnet.torch. However the accompanying\n",
    "      # code of 'Deep Residual Learning for Image Recognition' uses\n",
    "      # padding='VALID' for pool1. You can switch to that choice by setting\n",
    "      # slim.arg_scope([slim.max_pool2d], padding='VALID').\n",
    "      with slim.arg_scope([slim.max_pool2d], padding='SAME') as arg_sc:\n",
    "        return arg_sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "resnet_arg_scope = resnet_arg_scope\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "class NoOpScope(object):\n",
    "  \"\"\"No-op context manager.\"\"\"\n",
    "\n",
    "  def __enter__(self):\n",
    "    return None\n",
    "\n",
    "  def __exit__(self, exc_type, exc_value, traceback):\n",
    "    return False\n",
    "\n",
    "\n",
    "@slim.add_arg_scope\n",
    "def bottleneck(inputs,\n",
    "               depth,\n",
    "               depth_bottleneck,\n",
    "               stride,\n",
    "               rate=1,\n",
    "               outputs_collections=None,\n",
    "               scope=None,\n",
    "               use_bounded_activations=False):\n",
    "  \"\"\"Bottleneck residual unit variant with BN after convolutions.\n",
    "  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for\n",
    "  its definition. Note that we use here the bottleneck variant which has an\n",
    "  extra bottleneck layer.\n",
    "  When putting together two consecutive ResNet blocks that use this unit, one\n",
    "  should use stride = 2 in the last unit of the first block.\n",
    "  Args:\n",
    "    inputs: A tensor of size [batch, height, width, channels].\n",
    "    depth: The depth of the ResNet unit output.\n",
    "    depth_bottleneck: The depth of the bottleneck layers.\n",
    "    stride: The ResNet unit's stride. Determines the amount of downsampling of\n",
    "      the units output compared to its input.\n",
    "    rate: An integer, rate for atrous convolution.\n",
    "    outputs_collections: Collection to add the ResNet unit output.\n",
    "    scope: Optional variable_scope.\n",
    "    use_bounded_activations: Whether or not to use bounded activations. Bounded\n",
    "      activations better lend themselves to quantized inference.\n",
    "  Returns:\n",
    "    The ResNet unit's output.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'bottleneck_v1', [inputs]) as sc:\n",
    "    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n",
    "    if depth == depth_in:\n",
    "      shortcut = resnet_utils.subsample(inputs, stride, 'shortcut')\n",
    "    else:\n",
    "      shortcut = slim.conv2d(\n",
    "          inputs,\n",
    "          depth, [1, 1],\n",
    "          stride=stride,\n",
    "          activation_fn=tf.nn.relu6 if use_bounded_activations else None,\n",
    "          scope='shortcut')\n",
    "\n",
    "    residual = slim.conv2d(inputs, depth_bottleneck, [1, 1], stride=1,\n",
    "                           scope='conv1')\n",
    "    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,\n",
    "                                        rate=rate, scope='conv2')\n",
    "    residual = slim.conv2d(residual, depth, [1, 1], stride=1,\n",
    "                           activation_fn=None, scope='conv3')\n",
    "\n",
    "    if use_bounded_activations:\n",
    "      # Use clip_by_value to simulate bandpass activation.\n",
    "      residual = tf.clip_by_value(residual, -6.0, 6.0)\n",
    "      output = tf.nn.relu6(shortcut + residual)\n",
    "    else:\n",
    "      output = tf.nn.relu(shortcut + residual)\n",
    "\n",
    "    return slim.utils.collect_named_outputs(outputs_collections,\n",
    "                                            sc.name,\n",
    "                                            output)\n",
    "\n",
    "\n",
    "def resnet_v1(inputs,\n",
    "              blocks,\n",
    "              num_classes=None,\n",
    "              is_training=True,\n",
    "              global_pool=True,\n",
    "              output_stride=None,\n",
    "              include_root_block=True,\n",
    "              spatial_squeeze=True,\n",
    "              store_non_strided_activations=False,\n",
    "              reuse=None,\n",
    "              scope=None):\n",
    "  \"\"\"Generator for v1 ResNet models.\n",
    "  This function generates a family of ResNet v1 models. See the resnet_v1_*()\n",
    "  methods for specific model instantiations, obtained by selecting different\n",
    "  block instantiations that produce ResNets of various depths.\n",
    "  Training for image classification on Imagenet is usually done with [224, 224]\n",
    "  inputs, resulting in [7, 7] feature maps at the output of the last ResNet\n",
    "  block for the ResNets defined in [1] that have nominal stride equal to 32.\n",
    "  However, for dense prediction tasks we advise that one uses inputs with\n",
    "  spatial dimensions that are multiples of 32 plus 1, e.g., [321, 321]. In\n",
    "  this case the feature maps at the ResNet output will have spatial shape\n",
    "  [(height - 1) / output_stride + 1, (width - 1) / output_stride + 1]\n",
    "  and corners exactly aligned with the input image corners, which greatly\n",
    "  facilitates alignment of the features to the image. Using as input [225, 225]\n",
    "  images results in [8, 8] feature maps at the output of the last ResNet block.\n",
    "  For dense prediction tasks, the ResNet needs to run in fully-convolutional\n",
    "  (FCN) mode and global_pool needs to be set to False. The ResNets in [1, 2] all\n",
    "  have nominal stride equal to 32 and a good choice in FCN mode is to use\n",
    "  output_stride=16 in order to increase the density of the computed features at\n",
    "  small computational and memory overhead, cf. http://arxiv.org/abs/1606.00915.\n",
    "  Args:\n",
    "    inputs: A tensor of size [batch, height_in, width_in, channels].\n",
    "    blocks: A list of length equal to the number of ResNet blocks. Each element\n",
    "      is a resnet_utils.Block object describing the units in the block.\n",
    "    num_classes: Number of predicted classes for classification tasks.\n",
    "      If 0 or None, we return the features before the logit layer.\n",
    "    is_training: whether batch_norm layers are in training mode. If this is set\n",
    "      to None, the callers can specify slim.batch_norm's is_training parameter\n",
    "      from an outer slim.arg_scope.\n",
    "    global_pool: If True, we perform global average pooling before computing the\n",
    "      logits. Set to True for image classification, False for dense prediction.\n",
    "    output_stride: If None, then the output will be computed at the nominal\n",
    "      network stride. If output_stride is not None, it specifies the requested\n",
    "      ratio of input to output spatial resolution.\n",
    "    include_root_block: If True, include the initial convolution followed by\n",
    "      max-pooling, if False excludes it.\n",
    "    spatial_squeeze: if True, logits is of shape [B, C], if false logits is\n",
    "        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n",
    "        To use this parameter, the input images must be smaller than 300x300\n",
    "        pixels, in which case the output logit layer does not contain spatial\n",
    "        information and can be removed.\n",
    "    store_non_strided_activations: If True, we compute non-strided (undecimated)\n",
    "      activations at the last unit of each block and store them in the\n",
    "      `outputs_collections` before subsampling them. This gives us access to\n",
    "      higher resolution intermediate activations which are useful in some\n",
    "      dense prediction problems but increases 4x the computation and memory cost\n",
    "      at the last unit of each block.\n",
    "    reuse: whether or not the network and its variables should be reused. To be\n",
    "      able to reuse 'scope' must be given.\n",
    "    scope: Optional variable_scope.\n",
    "  Returns:\n",
    "    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].\n",
    "      If global_pool is False, then height_out and width_out are reduced by a\n",
    "      factor of output_stride compared to the respective height_in and width_in,\n",
    "      else both height_out and width_out equal one. If num_classes is 0 or None,\n",
    "      then net is the output of the last ResNet block, potentially after global\n",
    "      average pooling. If num_classes a non-zero integer, net contains the\n",
    "      pre-softmax activations.\n",
    "    end_points: A dictionary from components of the network to the corresponding\n",
    "      activation.\n",
    "  Raises:\n",
    "    ValueError: If the target output_stride is not valid.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope, 'resnet_v1', [inputs], reuse=reuse) as sc:\n",
    "    end_points_collection = sc.original_name_scope + '_end_points'\n",
    "    with slim.arg_scope([slim.conv2d, bottleneck,\n",
    "                         resnet_utils.stack_blocks_dense],\n",
    "                        outputs_collections=end_points_collection):\n",
    "      with (slim.arg_scope([slim.batch_norm], is_training=is_training)\n",
    "            if is_training is not None else NoOpScope()):\n",
    "        net = inputs\n",
    "        if include_root_block:\n",
    "          if output_stride is not None:\n",
    "            if output_stride % 4 != 0:\n",
    "              raise ValueError('The output_stride needs to be a multiple of 4.')\n",
    "            output_stride /= 4\n",
    "          net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope='conv1')\n",
    "          net = slim.max_pool2d(net, [3, 3], stride=2, scope='pool1')\n",
    "        net = resnet_utils.stack_blocks_dense(net, blocks, output_stride,\n",
    "                                              store_non_strided_activations)\n",
    "        # Convert end_points_collection into a dictionary of end_points.\n",
    "        end_points = slim.utils.convert_collection_to_dict(\n",
    "            end_points_collection)\n",
    "\n",
    "        if global_pool:\n",
    "          # Global average pooling.\n",
    "          net = tf.reduce_mean(net, [1, 2], name='pool5', keep_dims=True)\n",
    "          end_points['global_pool'] = net\n",
    "        if num_classes:\n",
    "          net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n",
    "                            normalizer_fn=None, scope='logits')\n",
    "          end_points[sc.name + '/logits'] = net\n",
    "          if spatial_squeeze:\n",
    "            net = tf.squeeze(net, [1, 2], name='SpatialSqueeze')\n",
    "            end_points[sc.name + '/spatial_squeeze'] = net\n",
    "          end_points['predictions'] = slim.softmax(net, scope='predictions')\n",
    "        return net, end_points\n",
    "resnet_v1.default_image_size = 224\n",
    "\n",
    "\n",
    "def resnet_v1_block(scope, base_depth, num_units, stride):\n",
    "  \"\"\"Helper function for creating a resnet_v1 bottleneck block.\n",
    "  Args:\n",
    "    scope: The scope of the block.\n",
    "    base_depth: The depth of the bottleneck layer for each unit.\n",
    "    num_units: The number of units in the block.\n",
    "    stride: The stride of the block, implemented as a stride in the last unit.\n",
    "      All other units have stride=1.\n",
    "  Returns:\n",
    "    A resnet_v1 bottleneck block.\n",
    "  \"\"\"\n",
    "  return resnet_utils.Block(scope, bottleneck, [{\n",
    "      'depth': base_depth * 4,\n",
    "      'depth_bottleneck': base_depth,\n",
    "      'stride': 1\n",
    "  }] * (num_units - 1) + [{\n",
    "      'depth': base_depth * 4,\n",
    "      'depth_bottleneck': base_depth,\n",
    "      'stride': stride\n",
    "  }])\n",
    "\n",
    "\n",
    "def resnet_v1_50(inputs,\n",
    "                 num_classes=None,\n",
    "                 is_training=True,\n",
    "                 global_pool=True,\n",
    "                 output_stride=None,\n",
    "                 spatial_squeeze=True,\n",
    "                 store_non_strided_activations=False,\n",
    "                 min_base_depth=8,\n",
    "                 depth_multiplier=1,\n",
    "                 reuse=None,\n",
    "                 scope='resnet_v1_50'):\n",
    "  \"\"\"ResNet-50 model of [1]. See resnet_v1() for arg and return description.\"\"\"\n",
    "  depth_func = lambda d: max(int(d * depth_multiplier), min_base_depth)\n",
    "  blocks = [\n",
    "      resnet_v1_block('block1', base_depth=depth_func(64), num_units=3,\n",
    "                      stride=2),\n",
    "      resnet_v1_block('block2', base_depth=depth_func(128), num_units=4,\n",
    "                      stride=2),\n",
    "      resnet_v1_block('block3', base_depth=depth_func(256), num_units=6,\n",
    "                      stride=2),\n",
    "      resnet_v1_block('block4', base_depth=depth_func(512), num_units=3,\n",
    "                      stride=1),\n",
    "  ]\n",
    "  return resnet_v1(inputs, blocks, num_classes, is_training,\n",
    "                   global_pool=global_pool, output_stride=output_stride,\n",
    "                   include_root_block=True, spatial_squeeze=spatial_squeeze,\n",
    "                   store_non_strided_activations=store_non_strided_activations,\n",
    "                   reuse=reuse, scope=scope)\n",
    "resnet_v1_50.default_image_size = resnet_v1.default_image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim.arg_scope('block1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_v1.resnet_v1_block('block1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.slim.python.slim.nets.resnet_utils import conv2d_same\n",
    "from tensorflow.contrib.slim.python.slim.nets.resnet_v1 import bottleneck, resnet_arg_scope\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "_RGB_MEAN = [123.68, 116.78, 103.94]\n",
    "\n",
    "def resnet(image, is_mem=False, is_training=False):\n",
    "  \"\"\" Send `image` through a ResNet50 with a non-local block at stage 3.\n",
    "  Use like so:\n",
    "      import nonlocal_resnet_v1_50_nl3 as model\n",
    "      endpoints, body_prefix = model.endpoints(images, is_training=True)\n",
    "      # BEFORE DEFINING THE OPTIMIZER:\n",
    "      model_variables = tf.get_collection(\n",
    "          tf.GraphKeys.GLOBAL_VARIABLES, body_prefix)\n",
    "      # IF LOADING PRE-TRAINED WEIGHTS:\n",
    "      saver = tf.train.Saver(mo////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////del_variables)\n",
    "      saver.restore(sess, args.initial_checkpoint)\n",
    "      # Do something with `endpoints['model_output']`\n",
    "  \"\"\"\n",
    "\n",
    "  if image.get_shape().ndims != 4:\n",
    "    raise ValueError('Input must be of size [batch, height, width, 3]')\n",
    "\n",
    "  image = image - tf.constant(_RGB_MEAN, dtype=tf.float32, shape=(1,1,1,3))\n",
    "\n",
    "  with tf.contrib.slim.arg_scope(resnet_arg_scope(batch_norm_decay=0.9, weight_decay=0.0)):\n",
    "    with tf.variable_scope('resnet_v1_50', values=[image]) as sc:\n",
    "      end_points_collection = sc.name + '_end_points'\n",
    "      with slim.arg_scope([slim.conv2d, bottleneck],\n",
    "                          outputs_collections=end_points_collection):\n",
    "        with slim.arg_scope([slim.batch_norm], is_training=is_training):\n",
    "          net = image\n",
    "          net = conv2d_same(net, 64, 7, stride=2, scope='conv1')\n",
    "          net = slim.max_pool2d(net, [3, 3], stride=2, scope='pool1')\n",
    "\n",
    "          # NOTE: base_depth is that inside the bottleneck. i/o is 4x that.\n",
    "          with tf.variable_scope('block1', values=[net]) as sc_block:\n",
    "            with tf.variable_scope('unit_1', values=[net]):\n",
    "              net = bottleneck(net, depth=4*64, depth_bottleneck=64, stride=1)\n",
    "            with tf.variable_scope('unit_2', values=[net]):\n",
    "              net = bottleneck(net, depth=4*64, depth_bottleneck=64, stride=1)\n",
    "            with tf.variable_scope('unit_3', values=[net]):\n",
    "              net = bottleneck(net, depth=4*64, depth_bottleneck=64, stride=2)\n",
    "\n",
    "          with tf.variable_scope('block2', values=[net]) as sc_block:\n",
    "            with tf.variable_scope('unit_1', values=[net]):\n",
    "              net = bottleneck(net, depth=4*128, depth_bottleneck=128, stride=1)\n",
    "            with tf.variable_scope('unit_2', values=[net]):\n",
    "              net = bottleneck(net, depth=4*128, depth_bottleneck=128, stride=1)\n",
    "            with tf.variable_scope('unit_3', values=[net]):\n",
    "              net = bottleneck(net, depth=4*128, depth_bottleneck=128, stride=1)\n",
    "            with tf.variable_scope('unit_4', values=[net]):\n",
    "              net = bottleneck(net, depth=4*128, depth_bottleneck=128, stride=2)\n",
    "\n",
    "          with tf.variable_scope('block3', values=[net]) as sc_block:\n",
    "            with tf.variable_scope('unit_1', values=[net]):\n",
    "              net = bottleneck(net, depth=4*256, depth_bottleneck=256, stride=1)\n",
    "            with tf.variable_scope('unit_2', values=[net]):\n",
    "              net = bottleneck(net, depth=4*256, depth_bottleneck=256, stride=1)\n",
    "            with tf.variable_scope('unit_3', values=[net]):\n",
    "              net = bottleneck(net, depth=4*256, depth_bottleneck=256, stride=1)\n",
    "            with tf.variable_scope('unit_4', values=[net]):\n",
    "              net = bottleneck(net, depth=4*256, depth_bottleneck=256, stride=1)\n",
    "            with tf.variable_scope('unit_5', values=[net]):\n",
    "              net = bottleneck(net, depth=4*256, depth_bottleneck=256, stride=1)\n",
    "            with tf.variable_scope('unit_6', values=[net]):\n",
    "              net = bottleneck(net, depth=4*256, depth_bottleneck=256, stride=2)\n",
    "            \n",
    "          with tf.variable_scope('block4', values=[net]) as sc_block:\n",
    "            with tf.variable_scope('unit_1', values=[net]):\n",
    "              net = bottleneck(net, depth=4*512, depth_bottleneck=512, stride=1)\n",
    "            with tf.variable_scope('unit_2', values=[net]):\n",
    "              net = bottleneck(net, depth=4*512, depth_bottleneck=512, stride=1)\n",
    "            with tf.variable_scope('unit_3', values=[net]):\n",
    "              net = bottleneck(net, depth=4*512, depth_bottleneck=512, stride=2)\n",
    "\n",
    "        # Global average pooling.\n",
    "        net = tf.reduce_mean(net, [1, 2], name='pool5', keep_dims=False)\n",
    "        # Convert end_points_collection into a dictionary of end_points.\n",
    "        endpts = slim.utils.convert_collection_to_dict(\n",
    "            end_points_collection)\n",
    "        endpts['model_output'] = endpts['global_pool'] = net\n",
    "\n",
    "    # The following is necessary to skip trying to load pre-trained non-local blocks.\n",
    "    return endpts, 'resnet_v1_50/(?!nonlocal)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = tf.placeholder(tf.float32, shape=[2,224,224,3])\n",
    "net, _ = resnet(ip, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
